{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg Workshop: Getting Started\n",
    "\n",
    "## How to run this workshop\n",
    "\n",
    "The workshop is consisted of several code cells that are designed to be executed from top to bottom. \n",
    "\n",
    "For example, this is the a code cell contains code to print \"Hello Iceberg Summit\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello Iceberg Summit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute a cell, click the cell and press Shift + Enter. You can also run the cell by clicking the \"Run the cell and advance\" button on the top (see the picture below)\n",
    "\n",
    "\n",
    "![](./imgs/jupyter-tutorial.png)\n",
    "\n",
    "The output will be displayed below the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg Table Structure\n",
    "\n",
    "<!-- ![](./imgs/iceberg-metadata.png){width=30px} -->\n",
    "\n",
    "<img src=\"./imgs/iceberg-metadata.png\" alt=\"Iceberg Metadata\" style=\"width:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catalog\n",
    "Catalog is responsible for managing and tracking iceberg tables, pointing to the current metadata file that represent a table's state.\n",
    "\n",
    "### Metadata files\n",
    "\n",
    "Table state is maintained in metadata files. All changes to table state create a new metadata file and replace the old metadata with an atomic swap. The table metadata file tracks the table schema, partitioning config, custom properties, and snapshots of the table contents. A snapshot represents the state of a table at some time and is used to access the complete set of data files in the table.\n",
    "\n",
    "### Data files\n",
    "Data files in snapshots are tracked by one or more manifest files that contain a row for each data file in the table, the file's partition data, and its metrics. The data in a snapshot is the union of all files in its manifests. Manifest files are reused across snapshots to avoid rewriting metadata that is slow-changing. Manifests can track data files with any subset of a table and are not associated with partitions.\n",
    "\n",
    "### Manifests and Manifest List\n",
    "The manifests that make up a snapshot are stored in a manifest list file. Each manifest list stores metadata about manifests, including partition stats and data file counts. These stats are used to avoid reading manifests that are not required for an operation.\n",
    "\n",
    "\n",
    "Ref: [Iceberg Table Spec Overview](https://iceberg.apache.org/spec/#overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Outline\n",
    "\n",
    "In the workshop, we are going to explore basic Iceberg table operations to introduce you to the essentials of Iceberg.\n",
    "\n",
    "We will use [PyIceberg](https://py.iceberg.apache.org/), a python implementation for accessing iceberg tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Catalog\n",
    "\n",
    "First, let's set up a catalog, which allows us to organize, manage, and track tables. Within a catalog, tables are grouped into logical containers called namespaces. You can visualize the structure of a catalog as follows:\n",
    "\n",
    "```\n",
    "catalog\n",
    "├── namespace_1\n",
    "│   ├── table_1\n",
    "│   └── table_2\n",
    "└── namespace_2\n",
    "    └── table_3\n",
    "```\n",
    "\n",
    "In this workshop, we'll start by creating a namespace called `demo_ns`, where we'll store all tables that we create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyiceberg.catalog import load_catalog\n",
    "\n",
    "catalog = load_catalog(\"default\")\n",
    "\n",
    "catalog.create_namespace_if_not_exists(\"demo_ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup for Re-runnability\n",
    "\n",
    "To ensure the workshop can be run multiple times without issues, we'll perform a cleanup step here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # In case the table already exists\n",
    "    catalog.drop_table(\"demo_ns.nyc_taxis\", True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Data: NYC Taxi Dataset\n",
    "\n",
    "In this workshop, we will use New York City Taxi & Limousine Commission's Trip Record Data, which can be downloaded from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "taxis_data_1 = pq.read_table('/home/jovyan/data/yellow_tripdata_2024-01.parquet')\n",
    "taxis_data_2 = pq.read_table('/home/jovyan/data/yellow_tripdata_2024-02.parquet')\n",
    "taxis_data_3 = pq.read_table('/home/jovyan/data/yellow_tripdata_2024-03.parquet')\n",
    "dataset_schema = taxis_data_1.schema\n",
    "dataset_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Iceberg table\n",
    "\n",
    "First, we'll create an iceberg table using the dataset's schema. Let's name the table `nyc_taxis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_NAME = \"demo_ns.nyc_taxis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxis_tbl = catalog.create_table(TABLE_NAME, schema=dataset_schema)\n",
    "nyc_taxis_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## What happens behind table creation?\n",
    "\n",
    "A metadata file has been created and registered as the latest metadata of table `demo_ns.nyc_taxis`. Let's login to Minio Bucket and see the file:\n",
    "\n",
    "- Minio Url: http://localhost:9001/\n",
    "- username: admin\n",
    "- password: password\n",
    "\n",
    "The table is created at [s3://warehouse/demo_ns/nyc_taxis](http://localhost:9001/browser/warehouse/demo_ns%2Fnyc_taxis%2F): \n",
    "\n",
    "![](./imgs/simple_table_create.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add data to the table\n",
    "\n",
    "The table we created is initially empty. Next, we'll add some data to the table to demonstrate how data insertion works and observe the resulting changes.\n",
    "\n",
    "We will `append` the `taxis_data_1` to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxis_tbl.append(taxis_data_1)\n",
    "nyc_taxis_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A new snapshot has been added\n",
    "\n",
    "From the output above, we can see a new snapshot has been created after the data insertion\n",
    "\n",
    "```\n",
    "snapshot: Operation.APPEND: id=<id>, schema_id=0\n",
    "```\n",
    "\n",
    "This snapshot indicates that the data change operation performed was an Append, matching the insert operation we executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the table\n",
    "\n",
    "To view the contents of the table, we'll read the data into a pandas DataFrame. For demonstration purposes, we'll limit our display to the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxis_tbl.scan(limit=10).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply filters and select specific columns from the final result. For example, we might want to view records that have 2 or more passengers and display only certain columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxis_tbl.scan(row_filter=\"passenger_count >= 2\", selected_fields=[\"VendorID\", \"passenger_count\", \"fare_amount\"], limit=10).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens when adding data?\n",
    "\n",
    "The data has been written into a parquet file and a new snapshot has been created.\n",
    "\n",
    "Let's check the table location again: [s3://warehouse/demo_ns/nyc_taxis](http://localhost:9001/browser/warehouse/demo_ns%2Fnyc_taxis%2F)\n",
    "\n",
    "We can see the table now have both `metadata` and `data`\n",
    "\n",
    "![](./imgs/simple_table_create_append_data.png)\n",
    "\n",
    "In the `metadata`, we can see some new files are generated\n",
    "\n",
    "![](./imgs/simple_table_create_append_data_new_metadata.png)\n",
    "\n",
    "In the `data`, we can see a new parquet file that contains the inserted data\n",
    "\n",
    "![](./imgs/simple_table_create_append_data_new_data.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Evolution: Make table partitioned\n",
    "\n",
    "The table we just created is unpartitioned. In this example, we want to take a further step to partition the table. We will partition the table by the `day` value of`tpep_pickup_datatime` column.\n",
    "\n",
    "We will use `update_spec` API to make the change, see [Partition Evolution](https://py.iceberg.apache.org/api/#partition-evolution) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.transforms import DayTransform\n",
    "\n",
    "with nyc_taxis_tbl.update_spec() as update_spec:\n",
    "    update_spec.add_field(\"tpep_pickup_datetime\", DayTransform())\n",
    "\n",
    "nyc_taxis_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above:\n",
    "```\n",
    "partition by: [tpep_pickup_datetime_day]\n",
    "```\n",
    "We can see that the table is now partitioned by `tpep_pickup_datetime`. The postfix `_day` indicates that the partitioning is based on the day portion of the datetime value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some new data to the table to see how data will be written into different partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxis_tbl.append(taxis_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxis_tbl.scan(limit=3).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioned Data\n",
    "\n",
    "If we go to the [`data` folder](http://localhost:9001/browser/warehouse/demo_ns%2Fnyc_taxis%2Fdata%2F) of table `nyc_taxis`:\n",
    "\n",
    "![](./imgs/partition-by-day.png)\n",
    "\n",
    "We can see that newly inserted data partitioned by date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folder name follows the format `<partition field>=<partition value>`\n",
    "\n",
    "For example, `tpep_pickup_datetime_day=2024-02-16`indicates that all data files within this folder contain records with a pickup date of `2024-02-16`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Evolution: Updating Partitioning to Month\n",
    "\n",
    "Suppose we now want future data insertions to partition the table by the `month` portion of the `tpep_pickup_datetime` column. Thankfully, Iceberg allows us to easily update the partitioning strategy without needing to recreate the table or rewrite existing data.\n",
    "\n",
    "Let's see how we can do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.transforms import MonthTransform\n",
    "\n",
    "with nyc_taxis_tbl.update_spec() as update_spec:\n",
    "    update_spec.remove_field(\"tpep_pickup_datetime_day\")\n",
    "    update_spec.add_field(\"tpep_pickup_datetime\", MonthTransform())\n",
    "\n",
    "nyc_taxis_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above:\n",
    "```\n",
    "partition by: [tpep_pickup_datetime_month]\n",
    "```\n",
    "We can see that the table is now partitioned by `tpep_pickup_datetime`. The postfix `_month` indicates that the partitioning is based on the month portion of the datetime value.\n",
    "\n",
    "Now let's append some new data to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxis_tbl.append(taxis_data_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we go to the the [`data` folder](http://localhost:9001/browser/warehouse/demo_ns%2Fnyc_taxis%2Fdata%2F) of table `nyc_taxis` again, we will find the new data is partitioned by the month value. (You can find folders of new partitions at the bottom)\n",
    "\n",
    "![](./imgs/partition-by-month.png)\n",
    "\n",
    "The previous day partitions' folders are still there because data inserted before partition spec change will remain in their original partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Evolution: Change Table Schema\n",
    "Iceberg supports schema evolution without rewriting any data. For example, we can rename `VendorId` to `ID`.\n",
    "\n",
    "We will use the `update_schema` API to achieve this, see [Schema evolution](https://py.iceberg.apache.org/api/#schema-evolution) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before rename\n",
    "nyc_taxis_tbl.scan(limit=3).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nyc_taxis_tbl.update_schema() as update:\n",
    "    update.rename_column(\"VendorID\", \"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After rename\n",
    "nyc_taxis_tbl.scan(limit=3).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata Table\n",
    "\n",
    "We can get more details of an iceberg by looking at its metadata tables. \n",
    "\n",
    "## Partitions\n",
    "For example, to learn about existing partitions in the table, we can query the `partitions` metadata table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxis_tbl.inspect.partitions().to_pandas().query(\"spec_id == 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files\n",
    "\n",
    "If we want to see all the data files in the table, we can query the `files` metadata table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxis_tbl.inspect.files().to_pandas().query(\"spec_id == 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapshots\n",
    "\n",
    "If we want to look at snapshots of the table, we can query the `snapshots` metadata table.\n",
    "\n",
    "Every time when a data change operation happens, Iceberg will form a new snapshot. In this example, we did 3 append and therefore we will have 3 snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxis_tbl.inspect.snapshots().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interoperability with other engines: Spark\n",
    "\n",
    "Iceberg tables provides engine/platform interoperability. In above example, we use PyIceberg to perform all table operations, we will show that the tables created by PyIceberg can also be consumed by Spark\n",
    "\n",
    "First, let's set a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "  .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "  .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.8.1,org.apache.iceberg:iceberg-aws-bundle:1.8.1\")\n",
    "  .config(\"spark.sql.catalog.demo.type\", \"rest\")\n",
    "  .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\")  \n",
    "  .config(\"spark.sql.catalog.demo.uri\", \"http://rest:8181\")\n",
    "  .config(\"spark.sql.catalog.demo.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")       \n",
    "  .config(\"spark.sql.catalog.demo.warehouse\", \"s3://warehouse\")\n",
    "  .config(\"spark.sql.catalog.demo.s3.endpoint\", \"http://minio:9000\")\n",
    "  .config(\"spark.sql.catalog.demo.s3.region\", \"us-east-1\")\n",
    "  .config(\"spark.sql.catalog.demo.s3.path-style-access\", \"true\")\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query the nyc_taxis table we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT ID, tpep_pickup_datetime, fare_amount FROM demo.demo_ns.nyc_taxis LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also query the metadata tables of nyc_taxis in spark. For examle, the `snapshots` metadata table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM demo.demo_ns.nyc_taxis.snapshots\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
