{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg Workshop: Getting Started\n",
    "\n",
    "## How to run this workshop\n",
    "\n",
    "The workshop is consisted of several code cells that are designed to be executed from top to bottom. \n",
    "\n",
    "For example, this is the a code cell contains code to print \"Hello Iceberg Summit\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello Iceberg Summit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute a cell, click it and press Shift + Enter. The output will be displayed below the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg Metadata Structure\n",
    "\n",
    "<!-- ![](./imgs/iceberg-metadata.png){width=30px} -->\n",
    "\n",
    "<img src=\"./imgs/iceberg-metadata.png\" alt=\"Iceberg Metadata\" style=\"width:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyiceberg.catalog import load_catalog\n",
    "\n",
    "catalog = load_catalog(\"default\")\n",
    "\n",
    "catalog.create_namespace_if_not_exists(\"demo_ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup To Ensure Re-runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # In case the table already exists\n",
    "    catalog.drop_table(\"demo_ns.nyc_taxis\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Data: NYC Taxi Dataset\n",
    "\n",
    "In this workshop, we will use New York City Taxi & Limousine Commission's Trip Record Data, which can be downloaded from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "taxis_data_1 = pq.read_table('/home/jovyan/data/yellow_tripdata_2024-01.parquet')\n",
    "taxis_data_2 = pq.read_table('/home/jovyan/data/yellow_tripdata_2024-02.parquet')\n",
    "taxis_data_3 = pq.read_table('/home/jovyan/data/yellow_tripdata_2024-03.parquet')\n",
    "dataset_schema = taxis_data_1.schema\n",
    "dataset_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Iceberg table\n",
    "\n",
    "First, we'll create an iceberg table using the dataset's schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_NAME = \"demo_ns.nyc_taxis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxis_tbl = catalog.create_table(TABLE_NAME, schema=dataset_schema)\n",
    "nyc_taxis_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## What happens behind table creation?\n",
    "\n",
    "A metadata file has been created and registered as the latest metadata of table `demo_ns.demo_table_1`. Let's login to Minio Bucket and see the file:\n",
    "\n",
    "- Minio Url: http://localhost:9001/\n",
    "- username: admin\n",
    "- password: password\n",
    "\n",
    "The table is created at [s3://warehouse/demo_ns/nyc_taxis](http://localhost:9001/browser/warehouse/demo_ns%2Fnyc_taxis%2F): \n",
    "\n",
    "![](./imgs/simple_table_create.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add data to the table\n",
    "\n",
    "It will create a new snapshot on the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxis_tbl.append(taxis_data_1)\n",
    "nyc_taxis_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the table\n",
    "\n",
    "We can see example data has been added to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxis_tbl.scan(limit=10).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens when adding data?\n",
    "\n",
    "The data has been written into a parquet file and a new snapshot has been created.\n",
    "\n",
    "Let's check the table location again: [s3://warehouse/demo_ns/nyc_taxis](http://localhost:9001/browser/warehouse/demo_ns%2Fnyc_taxis%2F)\n",
    "\n",
    "We can see the table now have both `metadata` and `data`\n",
    "\n",
    "![](./imgs/simple_table_create_append_data.png)\n",
    "\n",
    "In the `metadata`, we can see some new files are generated\n",
    "\n",
    "![](./imgs/simple_table_create_append_data_new_metadata.png)\n",
    "\n",
    "In the `data`, we can see a new parquet file that contains the inserted data\n",
    "\n",
    "![](./imgs/simple_table_create_append_data_new_data.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Evolution: Make table partitioned\n",
    "\n",
    "The table we just created is unpartitioned. In this example, we want to take a further step to partition the table. We will partition the table by the `day` value of`tpep_pickup_datatime` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.transforms import DayTransform\n",
    "\n",
    "with nyc_taxis_tbl.update_spec() as update_spec:\n",
    "    update_spec.add_field(\"tpep_pickup_datetime\", DayTransform())\n",
    "\n",
    "nyc_taxis_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxis_tbl.append(taxis_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxis_tbl.scan(limit=3).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxis_tbl.scan().to_pandas().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioned Data\n",
    "\n",
    "If we go to the [`data` folder](http://localhost:9001/browser/warehouse/demo_ns%2Fnyc_taxis%2Fdata%2F) of table `nyc_taxis`:\n",
    "\n",
    "![](./imgs/partition-by-day.png)\n",
    "\n",
    "We can see that newly inserted data partitioned by date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Evolution: Change to partition by month for future data insertion\n",
    "\n",
    "I changed my mind and now I want to partition the table by the \"month\" of `tpep_pickup_datetime` for any furture data insertion. No worriesâ€”we can easily achieve it!\n",
    "\n",
    "Iceberg allows you to update the partitioning strategy without recreating the table or re-writing any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.transforms import MonthTransform\n",
    "\n",
    "with nyc_taxis_tbl.update_spec() as update_spec:\n",
    "    update_spec.remove_field(\"tpep_pickup_datetime_day\")\n",
    "    update_spec.add_field(\"tpep_pickup_datetime\", MonthTransform())\n",
    "\n",
    "nyc_taxis_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's append some new data to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxis_tbl.append(taxis_data_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we go to the the [`data` folder](http://localhost:9001/browser/warehouse/demo_ns%2Fnyc_taxis%2Fdata%2F) of table `nyc_taxis` again, we will find the new data is partitioned by the month value. (You can find folders of new partitions at the bottom)\n",
    "\n",
    "![](./imgs/partition-by-month.png)\n",
    "\n",
    "The previous day partitions' folders are still there because data inserted before partition spec change will remain in their original partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Evolution: Change Table Schema\n",
    "Iceberg supports schema evolution without rewriting any data. For example, we can rename `VendorId` to `ID`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before rename\n",
    "nyc_taxis_tbl.scan(limit=3).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nyc_taxis_tbl.update_schema() as update:\n",
    "    update.rename_column(\"VendorID\", \"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After rename\n",
    "nyc_taxis_tbl.scan(limit=3).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata Table\n",
    "\n",
    "We can get more details of an iceberg by looking at its metadata tables. \n",
    "\n",
    "## Partitions\n",
    "For example, to learn about existing partitions in the table, we can query the `partitions` metadata table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxis_tbl.inspect.partitions().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files\n",
    "\n",
    "If we want to see all the data files in the table, we can query the `files` metadata table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxis_tbl.inspect.files().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapshots\n",
    "\n",
    "If we want to look at snapshots of the table, we can query the `snapshots` metadata table.\n",
    "\n",
    "Every time when a data change operation happens, Iceberg will form a new snapshot. In this example, we did 3 append and therefore we will have 2 snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxis_tbl.inspect.snapshots().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more metadata tables available, you can find more information here: https://iceberg.apache.org/docs/nightly/spark-queries/#inspecting-tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interoperability with other engines: Spark\n",
    "\n",
    "Iceberg tables provides engine/platform interoperability. In above example, we use PyIceberg to perform all table operations, we will show that the tables created by PyIceberg can also be consumed by Spark\n",
    "\n",
    "First, let's set a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "  .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "  .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.8.1,org.apache.iceberg:iceberg-aws-bundle:1.8.1\")\n",
    "  .config(\"spark.sql.catalog.demo.type\", \"rest\")\n",
    "  .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\")  \n",
    "  .config(\"spark.sql.catalog.demo.uri\", \"http://rest:8181\")\n",
    "  .config(\"spark.sql.catalog.demo.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")       \n",
    "  .config(\"spark.sql.catalog.demo.warehouse\", \"s3://warehouse\")\n",
    "  .config(\"spark.sql.catalog.demo.s3.endpoint\", \"http://minio:9000\")\n",
    "  .config(\"spark.sql.catalog.demo.s3.region\", \"us-east-1\")\n",
    "  .config(\"spark.sql.catalog.demo.s3.path-style-access\", \"true\")\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query the nyc_taxis table we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT ID, tpep_pickup_datetime, fare_amount FROM demo.demo_ns.nyc_taxis LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also query the metadata tables of nyc_taxis in spark. For examle, the `snapshots` metadata table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM demo.demo_ns.nyc_taxis.snapshots\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
