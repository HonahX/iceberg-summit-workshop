{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HonahX/iceberg-summit-workshop/blob/colab_dev/Iceberg_getting_started_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iceberg Workshop: Getting Started\n",
        "\n",
        "## How to run this workshop\n",
        "\n",
        "The workshop is consisted of several code cells that are designed to be executed from top to bottom.\n",
        "\n",
        "For example, this is the a code cell contains code to print \"Hello Iceberg Summit\"\n"
      ],
      "metadata": {
        "id": "j4gtVaQCff6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hello Iceberg Summit\")"
      ],
      "metadata": {
        "id": "MeZa44htflKy",
        "outputId": "e8e1ac3e-04a9-4d4e-f079-594927b4d274",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Iceberg Summit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To execute a cell, click it and press Shift + Enter. The output will be displayed below the cell."
      ],
      "metadata": {
        "id": "ItO59KBxffI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To execute a cell, click it and press Shift + Enter. The output will be displayed below the cell.\n",
        "\n",
        "# Iceberg Metadata Structure\n",
        "\n",
        "![My Image](https://github.com/HonahX/iceberg-summit-workshop/blob/main/notebooks/imgs/iceberg-metadata.png?raw=true)"
      ],
      "metadata": {
        "id": "R3RbmSYbg13E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "1di82M5GhVgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyiceberg[pyarrow,pandas,sql-sqlite]==0.9.0"
      ],
      "metadata": {
        "id": "XswQGbTw3V_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create utils to print directory"
      ],
      "metadata": {
        "id": "VtgjbW8U3WuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def print_directory(root_path, indent=''):\n",
        "    try:\n",
        "        entries = sorted(os.listdir(root_path))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"{indent}[Error] Path not found: {root_path}\")\n",
        "        return\n",
        "    except PermissionError:\n",
        "        print(f\"{indent}[Error] Permission denied: {root_path}\")\n",
        "        return\n",
        "\n",
        "    for i, entry in enumerate(entries):\n",
        "        path = os.path.join(root_path, entry)\n",
        "        is_last = (i == len(entries) - 1)\n",
        "        branch = '└── ' if is_last else '├── '\n",
        "        print(f\"{indent}{branch}{entry}\")\n",
        "        if os.path.isdir(path):\n",
        "            new_indent = indent + ('    ' if is_last else '│   ')\n",
        "            print_directory(path, new_indent)"
      ],
      "metadata": {
        "id": "FxilQBON1pv1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Example Data"
      ],
      "metadata": {
        "id": "cWlJI7xJ1uac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "data_dir = \"/data\"\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet -O /data/yellow_tripdata_2024-01.parquet\n",
        "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet -O /data/yellow_tripdata_2024-02.parquet\n",
        "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet -O /data/yellow_tripdata_2024-03.parquet"
      ],
      "metadata": {
        "id": "5VbmBr6c1xdr",
        "outputId": "a69ad132-d552-45bc-ca57-963cc8ceffaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-06 16:10:04--  https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\n",
            "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 18.160.201.126, 18.160.201.5, 18.160.201.131, ...\n",
            "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|18.160.201.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49961641 (48M) [binary/octet-stream]\n",
            "Saving to: ‘/data/yellow_tripdata_2024-01.parquet’\n",
            "\n",
            "/data/yellow_tripda 100%[===================>]  47.65M   144MB/s    in 0.3s    \n",
            "\n",
            "2025-04-06 16:10:04 (144 MB/s) - ‘/data/yellow_tripdata_2024-01.parquet’ saved [49961641/49961641]\n",
            "\n",
            "--2025-04-06 16:10:04--  https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet\n",
            "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 18.160.201.126, 18.160.201.5, 18.160.201.131, ...\n",
            "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|18.160.201.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50349284 (48M) [binary/octet-stream]\n",
            "Saving to: ‘/data/yellow_tripdata_2024-02.parquet’\n",
            "\n",
            "/data/yellow_tripda 100%[===================>]  48.02M   131MB/s    in 0.4s    \n",
            "\n",
            "2025-04-06 16:10:05 (131 MB/s) - ‘/data/yellow_tripdata_2024-02.parquet’ saved [50349284/50349284]\n",
            "\n",
            "--2025-04-06 16:10:05--  https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-03.parquet\n",
            "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 18.160.201.126, 18.160.201.5, 18.160.201.131, ...\n",
            "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|18.160.201.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 60078280 (57M) [binary/octet-stream]\n",
            "Saving to: ‘/data/yellow_tripdata_2024-03.parquet’\n",
            "\n",
            "/data/yellow_tripda 100%[===================>]  57.29M   168MB/s    in 0.3s    \n",
            "\n",
            "2025-04-06 16:10:05 (168 MB/s) - ‘/data/yellow_tripdata_2024-03.parquet’ saved [60078280/60078280]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Catalog"
      ],
      "metadata": {
        "id": "bdSqHiDT12e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyiceberg.catalog import load_catalog\n",
        "\n",
        "\n",
        "warehouse = \"/warehouse\"\n",
        "\n",
        "sqlite_uri = f\"sqlite:////{warehouse}/sql-catalog.db\"\n",
        "catalog = load_catalog(\"in-memory\", warehouse=warehouse, **{\n",
        "    \"uri\": \"sqlite:///:memory:\"\n",
        "})\n",
        "\n",
        "catalog.create_namespace_if_not_exists(\"demo_ns\")"
      ],
      "metadata": {
        "id": "i11gkoMfK2M5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleanup To Ensure Re-runnable"
      ],
      "metadata": {
        "id": "gP4YcBFJhVCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # In case the table already exists\n",
        "    catalog.drop_table(\"demo_ns.nyc_taxis\")\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "eHsGd8uokF-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Data: NYC Taxi Dataset\n",
        "\n",
        "In this workshop, we will use New York City Taxi & Limousine Commission's Trip Record Data, which can be downloaded from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
      ],
      "metadata": {
        "id": "Cp5Ydns4vmVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow.parquet as pq\n",
        "\n",
        "taxis_data_1 = pq.read_table('/data/yellow_tripdata_2024-01.parquet')\n",
        "taxis_data_2 = pq.read_table('/data/yellow_tripdata_2024-02.parquet')\n",
        "taxis_data_3 = pq.read_table('/data/yellow_tripdata_2024-03.parquet')\n",
        "dataset_schema = taxis_data_1.schema\n",
        "dataset_schema"
      ],
      "metadata": {
        "id": "1BfGXgI2shd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an Iceberg table\n",
        "\n",
        "First, we'll create an iceberg table using the dataset's schema."
      ],
      "metadata": {
        "id": "6-bqPWxCvqol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TABLE_NAME = \"demo_ns.nyc_taxis\""
      ],
      "metadata": {
        "id": "3p99Dc4svp-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_taxis_tbl = catalog.create_table(TABLE_NAME, schema=dataset_schema)\n",
        "nyc_taxis_tbl"
      ],
      "metadata": {
        "id": "ohsdERHFvWQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What happens behind table creation?\n",
        "\n",
        "A metadata file has been created and registered as the latest metadata of table `demo_ns.nyc_taxis`. Let's view the table's location."
      ],
      "metadata": {
        "id": "30gVbW4tvy--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_directory(nyc_taxis_tbl.location())"
      ],
      "metadata": {
        "id": "5YAHpZ-EvwJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add data to the table\n",
        "\n",
        "It will create a new snapshot on the table"
      ],
      "metadata": {
        "id": "SuaZBLc5wRdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_taxis_tbl.append(taxis_data_1)\n",
        "nyc_taxis_tbl"
      ],
      "metadata": {
        "id": "pGzsDQJwwI4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the table\n",
        "\n",
        "We can see example data has been added to the table"
      ],
      "metadata": {
        "id": "Yufezf97wXPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_taxis_tbl.scan(limit=10).to_pandas()"
      ],
      "metadata": {
        "id": "nuxowY3ywTNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What happens when adding data?\n",
        "\n",
        "The data has been written into a parquet file and a new snapshot has been created.\n",
        "\n",
        "Let's check the table location again:"
      ],
      "metadata": {
        "id": "doR_H0cswg4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_directory(nyc_taxis_tbl.location())"
      ],
      "metadata": {
        "id": "J9n3wWKGwWl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the `metadata`, we can see some new files are generated:\n",
        "\n",
        "\n",
        "*   new metadata file: `00001-<uuid>-.metadata.json`\n",
        "*   manifest file: `<uuid>-m0.avro`\n",
        "*   manifest list file: `snap-<snapshot-id>-0-<uuid>.avro`\n",
        "\n",
        "In the `data`, we can see a new parquet file that contains the inerted data\n",
        "\n",
        "\n",
        "\n",
        "*   `00000-0-<uuid>.parquet`\n",
        "\n"
      ],
      "metadata": {
        "id": "cVCTVTSzwoMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table Evolution: Make table partitioned\n",
        "\n",
        "The table we just created is unpartitioned. In this example, we want to take a further step to partition the table. We will partition the table by the `day` value of`tpep_pickup_datatime` column."
      ],
      "metadata": {
        "id": "vg1m_GGrxUyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyiceberg.transforms import DayTransform\n",
        "\n",
        "with nyc_taxis_tbl.update_spec() as update_spec:\n",
        "    update_spec.add_field(\"tpep_pickup_datetime\", DayTransform())\n",
        "\n",
        "nyc_taxis_tbl"
      ],
      "metadata": {
        "id": "HU77hrdUwjNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Insert new data\n",
        "\n",
        "The newly inserted data will be partitioned by the `day` value of `tpep_pickup_datetime` column"
      ],
      "metadata": {
        "id": "Fy1XNEFyxdlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_taxis_tbl.append(taxis_data_2)"
      ],
      "metadata": {
        "id": "cn8Xlg31xXS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_taxis_tbl.scan(limit=3).to_pandas()"
      ],
      "metadata": {
        "id": "-1XH0xuaxoaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_taxis_tbl.scan().to_pandas().size"
      ],
      "metadata": {
        "id": "F_tIvtNdxobj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partitioned Data\n",
        "\n",
        "If we go to the `data` folder of the table, we can see the newly inserted data partitioned by date."
      ],
      "metadata": {
        "id": "7odzbwVLxyT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_directory(os.path.join(nyc_taxis_tbl.location(), \"data\"))"
      ],
      "metadata": {
        "id": "xJO5fGNzxofQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table Evolution: Change to partition by month for future data insertion\n",
        "\n",
        "I changed my mind and now I want to partition the table by the \"month\" of `tpep_pickup_datetime` for any furture data insertion. No worries—we can easily achieve it!\n",
        "\n",
        "Iceberg allows you to update the partitioning strategy without recreating the table or re-writing any data."
      ],
      "metadata": {
        "id": "IVGM5-0NycBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyiceberg.transforms import MonthTransform\n",
        "\n",
        "with nyc_taxis_tbl.update_spec() as update_spec:\n",
        "    update_spec.remove_field(\"tpep_pickup_datetime_day\")\n",
        "    update_spec.add_field(\"tpep_pickup_datetime\", MonthTransform())\n",
        "\n",
        "nyc_taxis_tbl"
      ],
      "metadata": {
        "id": "bUdoO9rHx9pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's append some new data to the table"
      ],
      "metadata": {
        "id": "LMOY_9cHyimQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_taxis_tbl.append(taxis_data_3)"
      ],
      "metadata": {
        "id": "lqXSH5_ZyecZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we go to the the `data` folder of table `nyc_taxis` again, we will find the new data is partitioned by the month value. (You can find folders of new partitions at the bottom)\n",
        "\n",
        "```\n",
        "├── tpep_pickup_datetime_month=2002-12\n",
        "│   └── 00000-2-<uuid>.parquet\n",
        "├── tpep_pickup_datetime_month=2024-02\n",
        "│   └── 00000-1-<uuid>.parquet\n",
        "├── tpep_pickup_datetime_month=2024-03\n",
        "│   └── 00000-0-<uuid>.parquet\n",
        "└── tpep_pickup_datetime_month=2024-04\n",
        "    └── 00000-3-<uuid>.parquet\n",
        "```"
      ],
      "metadata": {
        "id": "GXOdSt6Gyouz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_directory(os.path.join(nyc_taxis_tbl.location(), \"data\"))"
      ],
      "metadata": {
        "id": "YWi3mljXykaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table Evolution: Change Table Schema\n",
        "Iceberg supports schema evolution without rewriting any data. For example, we can rename `VendorId` to `ID`.\n"
      ],
      "metadata": {
        "id": "aJyD5d1WzBc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before rename\n",
        "nyc_taxis_tbl.scan(limit=3).to_pandas()"
      ],
      "metadata": {
        "id": "_CtgcQgDyv7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with nyc_taxis_tbl.update_schema() as update:\n",
        "    update.rename_column(\"VendorID\", \"ID\")"
      ],
      "metadata": {
        "id": "FivhqS4bzDgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After rename\n",
        "nyc_taxis_tbl.scan(limit=3).to_pandas()"
      ],
      "metadata": {
        "id": "ixaatIb6zEnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metadata Table\n",
        "\n",
        "We can get more details of an iceberg by looking at its metadata tables.\n",
        "\n",
        "## Partitions\n",
        "For example, to learn about existing partitions in the table, we can query the `partitions` metadata table"
      ],
      "metadata": {
        "id": "rv2SJ1cEzHCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_taxis_tbl.inspect.partitions().to_pandas()"
      ],
      "metadata": {
        "id": "EvbAWJqlzKRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Files\n",
        "\n",
        "If we want to see all the data files in the table, we can query the `files` metadata table"
      ],
      "metadata": {
        "id": "BIFK0oDFzHE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_taxis_tbl.inspect.files().to_pandas()"
      ],
      "metadata": {
        "id": "hqRm21apzOTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Snapshots\n",
        "\n",
        "If we want to look at snapshots of the table, we can query the `snapshots` metadata table.\n",
        "\n",
        "Every time when a data change operation happens, Iceberg will form a new snapshot. In this example, we did 3 append and therefore we will have 3 snapshots"
      ],
      "metadata": {
        "id": "agQgTu-9zHLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_taxis_tbl.inspect.snapshots().to_pandas()"
      ],
      "metadata": {
        "id": "tuzsQMUjzbEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LKSEr8NNzHOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JYWemL-UzHQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v2HPmAM9zHS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "  .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
        "  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
        "  .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.8.1,org.apache.iceberg:iceberg-aws-bundle:1.8.1\")\n",
        "  .config(\"spark.sql.catalog.demo.type\", \"jdbc\")\n",
        "  .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
        "  .config(\"spark.sql.catalog.demo.uri\", \"jdbc:sqlite:///:memory:\")\n",
        "  .config(\"spark.sql.catalog.demo.warehouse\", warehouse)\n",
        ").getOrCreate()"
      ],
      "metadata": {
        "id": "QWtsiFA5zFr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT ID, tpep_pickup_datetime, fare_amount FROM demo.demo_ns.nyc_taxis LIMIT 5\").show()"
      ],
      "metadata": {
        "id": "J0NT7LEo0OUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0vHKeNWS0aPj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}